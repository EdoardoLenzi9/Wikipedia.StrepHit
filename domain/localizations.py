# Link
sparql_url = "https://query.wikidata.org/bigdata/namespace/wdq/sparql"
entity_url = "https://www.wikidata.org/wiki/Special:EntityData/"

# File
old_input_file = "assets/supervised_dataset_old.qs"
input_file = "assets/supervised_dataset.qs"
test_file = "assets/supervised_dataset_reduced.qs"
output_file = "assets/supervised_dataset_output.qs"
error_file =  "assets/supervised_dataset_errors.log"

source_mapping_file = "assets/supervised_dataset_source_mappings.json"
mapping_file = "assets/supervised_dataset_mappings.json"
unknown_mapping_file = "assets/supervised_dataset_unknown_mappings.json"
refreshed_urls_file = "assets/supervised_dataset_refreshed_urls.json"
deleted_rows_file = "assets/supervised_dataset_deleted_rows.log"

# Const
LOAD_MAPPINGS = True            # loads autogenerated mappings from "assets/supervised_dataset_mappings.json" and "assets/supervised_dataset_unknown_mappings.json" files
MAP_ALL_RESPONSES = False       # when you call add_references procedure, in case of an unmapped 'reference URL' (P854) inserts in "supervised_dataset_mappings.json"  record of the result sparql-query (business/queries/sitelink_queries.py).
IS_ASYNC_MODE = False           # when you call add_references procedure,processes each row on a new thread.
DELETE_ROW = True               # when you call refresh procedure, deletes rows with unrechable 'reference URL' (P854) (must have stable internet connection!)
REFRESH_UNKNOWN_DOMAINS = True  # when you call refresh procedure, replaces old urls with updated urls in case of site redirection.

domains_to_refresh = [  "www.bbc.co.uk",  
                        "archive.org", 
                        "artuk.org", 
                    ]

domains_to_https =  [   "www.metal-archives.com",
                        "www.wga.hu"
                    ]